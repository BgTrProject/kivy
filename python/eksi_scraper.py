# -*- coding: utf-8 -*-
"""eksi_scraper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GsXEB6ly8n5p3WqTN47FzeGqJMhIJtrc
"""

# !pip3 install beautifulsoup4
# !pip3 install selenium
# !pip3 install pymongo==3.11.2

import time
import json
from collections import defaultdict
import pandas as pd
import json
import csv
import dtale
from pandas.io.json import json_normalize


from selenium import webdriver
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.remote.webelement import WebElement
from selenium.webdriver import ActionChains
import time
from selenium.webdriver.common.keys import Keys
# from pymongo import MongoClient
import sys
import os
# import environ
#
# env = environ.Env()
# env.read_env(env.str('ENV_PATH', '.env'))
#
# mongo_cli_username = os.environ.get('MONGO_CLI_USERNAME')
# mongo_cli_password = os.environ.get('MONGO_CLI_PASSWORD')

# target url
# url = "https://eksisozluk.com/"
#
# # input location
# input_location = 'data/input/keywords.txt'
#
# # output location
# output_location = 'data/output/eksi.json'

# client = MongoClient("mongodb+srv://{}:{}@cluster0.plop5.mongodb.net/myFirstDatabase?retryWrites=true&w=majority".format(mongo_cli_username, mongo_cli_password))
# db = client['healdash']

# keywords list
# keywords = []
#
# with open(input_location) as my_file:
#     for line in my_file:
#         keywords.append(line.replace("\n", ""))

# show keywords
# keywords

# class structure
class Eksi:
    def __init__(self, url: str,filname) -> None:
        self.url = url
        self.filname=filname
        self.filname3=filname
        desktop = os.path.join(os.path.join(os.path.expanduser('~')), 'Desktop')
        if os.getcwd().startswith('/home'):
            self.filname3 = desktop + "/" + self.filname + '.csv'
        else:
            self.filname3 = desktop + "//" + self.filname + '.csv'

        # init the browser
        # options = webdriver.ChromeOptions()
        # options.add_argument('--ignore-certificate-errors')
        # options.add_argument('--incognito')
        # options.add_argument('--headless')
        # options.w3c = True

        driver = webdriver.Chrome('/usr/bin/chromedriver')




        # driver = webdriver.Chrome() # initialize the driver
        driver.get(self.url) # go to the url
        self.driver = driver
        
    def search_keyword(self, keyword: str) -> None:
        # self.keyword=keyword
        driver=self.driver
        time.sleep(1)
        search_input = driver.find_element_by_id('search-textbox')




        print("**********************")
        print(keyword)
        print("?????????????????????????")
        # search_input.click()
        search_input.clear()
        # search_input.send_keys(Keys.HOME)
        search_input.send_keys(keyword)
        search_input.submit()
        time.sleep(0.2) # small delay before getting the page source
        
    def compile_page_source(self) -> object:
        page_source = self.driver.page_source # get the page source
        soup = BeautifulSoup(page_source.encode('utf-8','ignore')) # compile it with bs4
        try:
            self.max_pages = int(soup.find('div', {"class": "pager"})['data-pagecount'])
        except:
            self.max_pages = 1
        self.keyword_scape_time = self.max_pages * 0.5
        self.page_source = soup
        self.keyword_exists() # detect if the keyword exists
        return self
    
    def next_page(self, page_number: int) -> None:
        current_url = self.driver.current_url 
        current_url = current_url[:current_url.rfind("?")+1] # remove all url variables 
        
        # if there are not parameters in the existing url
        if not current_url:
            current_url = self.driver.current_url + "?"
            
        current_url = current_url + ('p={}'.format(page_number))
        self.driver.get(current_url)
        
    def clean_entry(self, entry: str) -> str: 
        return (
            entry
            .replace("\n", "") # remove new lines
            .replace("\'", "'") # fix apostrophe
            .strip() # remove spaces
        )
    
    def keyword_exists(self) -> bool:
        all_authors = self.page_source.find_all('a', {"class": "entry-author"}) # get all authors
        if len(all_authors):
            return True
        else:
            return False
        
    def scrape_data(self, keyword: str) -> None:
        # desktop = os.path.join(os.path.join(os.path.expanduser('~')), 'Desktop')
        # filname3 = desktop + "/" + self.filname + '.csv'
        sf=self.filname3
        print("1111111111111111111111111111111")
        print(self.filname3)
        print("1111111111111111111111111111111")
        all_entries = self.page_source.find_all('div', {"class": "content"}) # get all entries
        all_dates = self.page_source.find_all('a', {"class": "entry-date"}) # get all dates
        all_authors = self.page_source.find_all('a', {"class": "entry-author"}) # get all authors

        for a,b,c in zip(all_dates,all_authors,all_entries):
            with open(sf, 'a') as fff:
                w = csv.writer(fff)
                # w.writeheader()
                w.writerow([a.text.strip(),b.text.strip(),self.clean_entry(c.text.strip())])
            print("++++++++++++++++++++")
        for entry, date, author in zip(all_entries, all_dates, all_authors):
            self.keyword_dict[keyword].append({
                "date": date.text, 
                "author": author.text, 
                "entry": self.clean_entry(entry.text)
            })

          
    def scrape_all_pages(self, keyword_list: list) -> None:

        # reset keywords dict
        self.keyword_dict = defaultdict(list)
        
        for keyword in keyword_list:
            print("=================")
            print(keyword)
            print("=================")
            self.search_keyword(keyword)
            self.compile_page_source() # compile for the first time
            
            if self.keyword_exists:
                print("{} - scraping time: {} seconds".format(keyword, self.keyword_scape_time)) # print scraping time for the keyword

                for i in range(1, self.max_pages + 1):
                    self.next_page(i)
                    self.compile_page_source().scrape_data(keyword)
                    
                # add data to mongodb
                # db.eksi_entries.update_many({"keyword": keyword}, {"$set": {"objects": self.keyword_dict[keyword]}}, upsert=True)
            else:
                print("No results for {}".format(keyword))
                
    def get_json_output(self, filname: str) -> None:


        desktop = os.path.join(os.path.join(os.path.expanduser('~')), 'Desktop')
        print(desktop)
        filname2 = desktop + "/" + filname + '.json'
        print("--------*******************-------------")
        print(filname2)
        print("-----------****************----------")
        # print(filname2)
        # filname3 = desktop + "/" + filname + '.csv'
        # field_names=['date','author','entry']
        # with open (filname3,'a') as ff:
        #     w=csv.DictWriter(ff,fieldnames=field_names)
        #     w.writeheader()
        #     w.writerow(self.keyword_dict)
        # print("csv ok ")


        # dump the json file
        json_object = json.dumps(self.keyword_dict, ensure_ascii=False).encode('utf-8','ignore').decode()
        with open(filname2, 'a', encoding='utf-8') as f:
            json.dump(json_object, f, ensure_ascii=False)
        print("******json completed")


    def start(self,url,search_word,filname):
        self.url=url
        self.search_word=search_word
        self.filname=filname

        a = search_word.split(",")
        keywords = []
        for i in a:
            print("---------------------")
            print(i)
            print("---------------------")
            keywords.append(i)
        print("************************")
        print(type(keywords))
        print(keywords)
        print("************************")

        self.driver.close()
        eksi=Eksi(self.url,filname)

        print("//////////////////////////////")
        print(self.filname3)
        # self.filname3(self)
        print("///////////////// filname3 /////////////")
        field_names = ['date', 'author', 'entry']
        # save csv.........................................
        with open(self.filname3, 'w') as ff:
            w = csv.writer(ff)
            # w.writeheader()
            w.writerow(field_names)

        eksi.scrape_all_pages(keywords)
        eksi.get_json_output(self.filname)
        df = pd.read_csv(self.filname3)
        d = dtale.show(df)
        d.open_browser()



# initialize the object
# eksi = Eksi(url)

# scrape the data
# eksi.scrape_all_pages(keywords)

# get json output
# eksi.get_json_output(output_location)

# example
# eksi.keyword_dict